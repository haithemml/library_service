{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9288297,"sourceType":"datasetVersion","datasetId":5622763}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-13T16:27:42.314767Z","iopub.execute_input":"2024-09-13T16:27:42.315250Z","iopub.status.idle":"2024-09-13T16:27:42.824648Z","shell.execute_reply.started":"2024-09-13T16:27:42.315201Z","shell.execute_reply":"2024-09-13T16:27:42.823161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/library-services-dataset/Library_Services_20240831.csv')\nprint(data.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-13T16:28:39.684608Z","iopub.execute_input":"2024-09-13T16:28:39.685172Z","iopub.status.idle":"2024-09-13T16:28:39.744595Z","shell.execute_reply.started":"2024-09-13T16:28:39.685119Z","shell.execute_reply":"2024-09-13T16:28:39.743300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'include only \"object\" type columns in the statistical summary.","metadata":{}},{"cell_type":"code","source":"print(data.describe(include=\"object\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-13T16:31:51.125032Z","iopub.execute_input":"2024-09-13T16:31:51.125535Z","iopub.status.idle":"2024-09-13T16:31:51.248299Z","shell.execute_reply.started":"2024-09-13T16:31:51.125493Z","shell.execute_reply":"2024-09-13T16:31:51.247080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that in the 21 rows, there are 21 unique branch, 21 unique address, 13 unique city, and one unique charging.Some branch are very common. the same number for freq and unique in Location","metadata":{}},{"cell_type":"code","source":"data['input'] = 'TEXT1: ' + data.Location + '; TEXT2: ' + data.Address + '; ANC1: ' + data.Branch\nprint(data.input.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-13T16:55:02.663610Z","iopub.execute_input":"2024-09-13T16:55:02.664049Z","iopub.status.idle":"2024-09-13T16:55:02.673878Z","shell.execute_reply.started":"2024-09-13T16:55:02.664002Z","shell.execute_reply":"2024-09-13T16:55:02.672201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we use Transformers it's a Dataset object to store a set of data, We can create one:","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset,DatasetDict\n\ndataset = Dataset.from_pandas(data)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T16:59:51.093934Z","iopub.execute_input":"2024-09-13T16:59:51.095041Z","iopub.status.idle":"2024-09-13T16:59:51.253842Z","shell.execute_reply.started":"2024-09-13T16:59:51.094977Z","shell.execute_reply":"2024-09-13T16:59:51.252680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenization: Split each text up into words (or actually, as we'll see, into tokens)\nNumericalization: Convert each word (or token) into a number.\nFor each text in the columns that contain textual information, we will divide the text into words or \"tokens\". Here is an example of using nltk or spacy for tokenizer:","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\n# Downloading the necessary resources from nltk\nnltk.download('punkt')\n\n# Suppose the \"Webpage\" column contains text\ndata['tokens'] = data['Webpage'].apply(lambda x: word_tokenize(x.lower()))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-13T17:10:07.064218Z","iopub.execute_input":"2024-09-13T17:10:07.064792Z","iopub.status.idle":"2024-09-13T17:10:08.942955Z","shell.execute_reply.started":"2024-09-13T17:10:07.064744Z","shell.execute_reply":"2024-09-13T17:10:08.941603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will convert each token into a unique number. To do this, we can use a dictionary which associates each token with a unique digital identifier.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Initializing the encoder\nlabel_encoder = LabelEncoder()\n\n# Encoding of tokens\ndata['tokens_encoded'] = data['tokens'].apply(lambda x: label_encoder.fit_transform(x))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-13T17:12:02.794875Z","iopub.execute_input":"2024-09-13T17:12:02.796081Z","iopub.status.idle":"2024-09-13T17:12:02.807978Z","shell.execute_reply.started":"2024-09-13T17:12:02.795998Z","shell.execute_reply":"2024-09-13T17:12:02.806540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the columns that need to be tokenized and digitized. Columns like \"Branch\", \"Address\", or \"City\" could contain textual data to tokenize. You can filter these columns before launching the tokenization and numericalization process.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom sklearn.preprocessing import LabelEncoder\n\n# load data\ndf = {\n    'Branch': ['Branch A', 'Branch B'],\n    'Webpage': ['https://branchA.com', 'https://branchB.com'],\n    'Address': ['123 Main St', '456 Oak St'],\n    'City': ['New York', 'Los Angeles']\n}\ndata = pd.DataFrame(df)\n\n# Tokenization of text columns\ntext_columns = ['Branch', 'Webpage', 'Address', 'City']\nfor col in text_columns:\n    data[f'{col}_tokens'] = data[col].apply(lambda x: word_tokenize(x.lower()))\n\n# Numericalization\nlabel_encoder = LabelEncoder()\nfor col in text_columns:\n    data[f'{col}_tokens_encoded'] = data[f'{col}_tokens'].apply(lambda x: label_encoder.fit_transform(x))\n\n# Display result\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-13T17:16:20.164406Z","iopub.execute_input":"2024-09-13T17:16:20.164875Z","iopub.status.idle":"2024-09-13T17:16:20.184886Z","shell.execute_reply.started":"2024-09-13T17:16:20.164835Z","shell.execute_reply":"2024-09-13T17:16:20.183448Z"},"trusted":true},"execution_count":null,"outputs":[]}]}